---
layout: post
title: Backward 总结
---

完成 cs231n assignment 2 中 `bachnorm_backward` 函数花费了不少时间, 稍微总结下.

计算 Backward 主要分为两种方法 1). Computation graph 2). On paper

本质是一样的, 都是利用 Chain Rule. 但我个人更偏爱方法一, 对于复杂的函数会更清晰,
尤其是当我们在处理矩阵时.

下面我们以计算方差 `np.var(x, axis=0)` 为例来说明两种方法.

### Computation graph

首先我们把计算过程变成图, 图的节点为操作数或操作符. 尽量把计算过程分解成比较简单的运算.

![computation graph](/images/20170330.png)

然后就是按照从后往前的顺序一步步计算. 这里有几点要注意的:

1. 因为都是简单操作, 所以我们可以尝试根据 `shape` 来思考.

    比如 `x`, `w`, `dout` 的 `shape` 分别为 `(N, D)`, `(D, M)` 和 `(N, M)`,
    因为 `dx` 的 `shape` 应该和 `x` 一样为 `(N, D)`, 所以我们就可以得出 `dx = dout.dot(w.T)`.
    (假设这里 `out = x.dot(w)`)

2. input 中的某一个值如果参与到 output 中多个位置的计算, 记得把这几个位置 backward 的结果加起来.

3. backward 得到的结果表示的是 input 中相同位置的值对于最终的值的导数.

下面我们就一个个来计算:

``` python
dsquare = dvar / x.sahpe[0]
ddiff = 2 * (x - mean) * dsquare
dmean = -np.sum(ddiff, axis=0)
dx1 = ddiff
dx2 = dmean / x.shape[0]
dx = dx1 + dx2
```

### On paper

就是利用 Chain rule 来运算.

因为

$$
\begin{align*}
\frac{dout}{dx} =
\begin{pmatrix}
   \frac{dout}{dx_{00}} & .. & \frac{dout}}{dx_{0D}}} \\
   .. & \frac{dout}{dx_{ij}} & .. \\
   \frac{dout}{dx_{N0}} & ... & \frac{dout}{dx_{ND}}
\end{pmatrix}.
\end{align*}
$$

同时

$$
\begin{align*}
\frac{dout}{dvar} =
\begin{pmatrix}
   \frac{dout}{dvar_{0}} & .. & \frac{dout}}{dvar_{D}}}
\end{pmatrix}.
\end{align*}
$$

所以

$$
\begin{align*}
\frac{dout}{dx_ij} = \sum_{k} \frac{dout}{dvar_k} \cdot \frac{dvar_k}{dx_ij}
\end{align*}
$$

下面我们就来一步步推导:

$$
\begin{align*}
\frac{dvar_j}{dx_ij} &= \frac{d\frac{(x_0j - \bar{x})^2 + ... + (x_ij - \bar{x})^2 +... + (x_Nj - \bar{x})^2}{N}}{dx_ij} \\
&= \frac{2}{N} ((x_ij - \bar{x}) + \sum_k (x_kj - \bar{x})\frac{d\bar{x}}{dx_ij})
\because \sum_k (x_kj - \bar{x}) = 0
\therefore \frac{dvar_j}{dx_ij} = \frac{2(x_ij - \bar{x})}{N}
\end{align*}
$$

最后就是看怎么推广到矩阵. 这个步骤极容易出错, 这也是为什么我偏爱用 computation graph 的原因!

> [Backpropagation, Intuitions](http://cs231n.github.io/optimization-2/)
